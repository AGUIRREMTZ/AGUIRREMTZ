import cv2
import numpy as np
from keras.models import load_model

# Cargar el modelo preentrenado de reconocimiento de gestos
modelo_gestos = load_model("modelo_gestos.h5")

# Clases de gestos reconocidas por el modelo
gestos = ["Pu침o", "Palma Abierta", "Dedo Apuntando", "Ok", "Paz"]

# Inicializar el clasificador de detecci칩n de manos de OpenCV
clasificador_manos = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_hand.xml")

# Capturar video en tiempo real
video = cv2.VideoCapture(0)

while True:
    ret, frame = video.read()
    if not ret:
        break

    gris = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    # Detectar manos en la imagen
    manos = clasificador_manos.detectMultiScale(gris, scaleFactor=1.3, minNeighbors=5)

    for (x, y, w, h) in manos:
        # Recortar la regi칩n de la mano y redimensionarla para el modelo
        mano = gris[y:y + h, x:x + w]
        mano = cv2.resize(mano, (64, 64))
        mano = np.expand_dims(mano, axis=0)  # Agregar batch dimension
        mano = np.expand_dims(mano, axis=-1)  # Agregar canal de color (1 para gris)

        # Predecir el gesto con la CNN
        prediccion = modelo_gestos.predict(mano)
        gesto = gestos[np.argmax(prediccion)]  # Obtener el gesto con mayor probabilidad

        # Dibujar rect치ngulo y mostrar el gesto detectado
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(frame, gesto, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

    # Mostrar el resultado
    cv2.imshow("Reconocimiento de Gestos", frame)

    # Salir con la tecla 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Liberar recursos
video.release()
cv2.destroyAllWindows()
